# DLI: Deep Learning Inference Benchmark

## Введение

Перед разработчиками встает задача определения производительности железа в задаче исполнения глубоких моделей. 
Например, хочется решить проблему анализа пола-возраста покупателей, которые заходят в магазин, чтобы в зависимости от этого менять оформление магазина или наполнение товаром. 
Вы уже знаете какие модели будете использовать в вашем ПО, но до конца не понятно как выбрать железо. 
Можно выбрать самый топ и переплачивать как за простаивающие мощности, так и за электроэнергию.
Можно взять самый дешевый i3 и потом вдруг окажется, что он недостаточен для того, чтобы вывезти каскад из нескольких глубоких моделей на 8 камерах. 
А может быть камера всего одна, и для решения задачи достаточно Raspberry Pi с Movidius Neural Compute Stick? 
Поэтому хочется иметь инструмент для оценки скорости работы вашего инференса на разном железе.

## DLI benchmark 

Студентами ННГУ им. Н.И.Лобачевского был разработан открытый фреймворк DLI (https://github.com/itlab-vision/openvino-dl-benchmark), который позволяет запустить измерение производительности большого количества разных моделей на доступном железе.

## Как считать FPS


 
## Архитектура фреймворка



## Результаты



Мы проводили эксперименты на следующих машинах:

Результаты производительности моделей собраны в HTML-таблицы, посмотреть их можно здесь:
http://hpc-education.unn.ru/en/dli
Пока мы измеряли только производительность моделей, сконвертированных в OpenVINO. Уже готов код для тестирования моделей Caffe, и в разработке другие библиотеки глубокого обучения. 
Примеры анализа результатов

## Ссылки


